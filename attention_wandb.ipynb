{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipnarayan501/Assignment3/blob/main/attention_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq Attention with attention"
      ],
      "metadata": {
        "id": "czGkgu_TlnDM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KWbEGfwg0EHz"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import SimpleRNN,LSTM,GRU,Embedding,Dense,Dropout,Input,Concatenate\n",
        "from tensorflow.keras.optimizers import Adam,Nadam\n",
        "from keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyOULlor0N7o",
        "outputId": "e16427a1-3b41-4c1d-8f07-234bd9b6d629"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configuring wandb\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "lKKQTqJa2dKU",
        "outputId": "bb876600-3a12-4411-db22-811479deada4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 52.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"drive/MyDrive/hi.zip\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q hi.zip"
      ],
      "metadata": {
        "id": "aIik4Y_j0N93"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading dataset as dataframe and returning it\n",
        "def load_data(path):\n",
        "    with open(path) as file:\n",
        "        data = pd.read_csv(file,sep='\\t',header=None,names=[\"hi\",\"en\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[data['hi'].notna()]\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ],
      "metadata": {
        "id": "tH1g08_-0N5D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting dataset for train test validation\n",
        "train = load_data(\"hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "dev = load_data(\"hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
        "test = load_data(\"hi/lexicons/hi.translit.sampled.test.tsv\")"
      ],
      "metadata": {
        "id": "lkRr87yV0OAR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train['en'].values\n",
        "y = train['hi'].values\n",
        "y = '\\t'+y+'\\n'"
      ],
      "metadata": {
        "id": "2azwf5py_SpT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Unique tokens hindi and english language"
      ],
      "metadata": {
        "id": "0SBJ-jZzcUPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_tokenize(data):\n",
        "    english = train['en'].values\n",
        "    hindi = train['hi'].values\n",
        "    hindi = '\\t'+hindi+'\\n'\n",
        "    english_tokens = set()\n",
        "    hindi_tokens = set()\n",
        "    \n",
        "    for x,y in zip(english,hindi):\n",
        "        for ch in x:\n",
        "            english_tokens.add(ch)\n",
        "        for ch in y:\n",
        "            hindi_tokens.add(ch)\n",
        "    english_tokens = sorted(list(english_tokens))\n",
        "    hindi_tokens = sorted(list(hindi_tokens))\n",
        "    return hindi_tokens , english_tokens\n",
        "hindi_tokens , english_tokens = unique_tokenize(train)"
      ],
      "metadata": {
        "id": "iViQ7hPk-vyz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the tokens for hindi and engliish"
      ],
      "metadata": {
        "id": "tmWLA_kacieE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_map(hindi_tokens , english_tokens):\n",
        "    eng_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n",
        "    hin_token_map = dict([(ch,i+1) for i,ch in enumerate(hindi_tokens)])\n",
        "    hin_token_map[\" \"] = 0\n",
        "    eng_token_map[\" \"] = 0\n",
        "    return hin_token_map, eng_token_map\n",
        "\n",
        "hin_token_map, eng_token_map = tokenize_map(hindi_tokens , english_tokens)"
      ],
      "metadata": {
        "id": "t9ZAw7tP-v1N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting max length\n",
        "max_eng_len = max([len(i) for i in x])\n",
        "max_hin_len = max([len(i) for i in y])"
      ],
      "metadata": {
        "id": "8jgWmEEE-v3g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the datset"
      ],
      "metadata": {
        "id": "Kwjz_JDuc5Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(data):\n",
        "    x,y = data['en'].values, data['hi'].values\n",
        "    y = \"\\t\" + y + \"\\n\"\n",
        "    \n",
        "    a = np.zeros((len(x),max_eng_len),dtype=\"float32\")\n",
        "    b = np.zeros((len(y),max_hin_len),dtype=\"float32\")\n",
        "    c = np.zeros((len(y),max_hin_len,len(hindi_tokens)+1),dtype=\"int\")\n",
        "    \n",
        "    \n",
        "    for i,(xx,yy) in enumerate(zip(x,y)):\n",
        "        for j,ch in enumerate(xx):\n",
        "            a[i,j] = eng_token_map[ch]\n",
        "\n",
        "        a[i,j+1:] = eng_token_map[\" \"]\n",
        "        for j,ch in enumerate(yy):\n",
        "            b[i,j] = hin_token_map[ch]\n",
        "\n",
        "            if j>0:\n",
        "                c[i,j-1,hin_token_map[ch]] = 1\n",
        "\n",
        "        b[i,j+1:] = hin_token_map[\" \"]\n",
        "        c[i,j:,hin_token_map[\" \"]] = 1\n",
        "        \n",
        "    return a,b,c"
      ],
      "metadata": {
        "id": "-7f3sFCP1OTU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting preprocess train test and validation data\n",
        "trainx, trainxx, trainy = process(train)\n",
        "valx, valxx, valy = process(dev)\n",
        "testx,testxx,testy = process(test)"
      ],
      "metadata": {
        "id": "HmCHNwFl1OVZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iqtTtYLC2mQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "reverse_eng_map = dict([(i,char) for char,i in eng_token_map.items()])\n",
        "reverse_hin_map = dict([(i,char) for char,i in hin_token_map.items()])"
      ],
      "metadata": {
        "id": "1IggKGuw1OXo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TLMpyU421lv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention code"
      ],
      "metadata": {
        "id": "AXiL_KGTdNHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#returns context vector and attention weights\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.units = units\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "  \n",
        "        })\n",
        "        return config\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # score \n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector \n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "Ah2mT5F1c0Qo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating encoder decoder models\n",
        "def build_model(cell = \"LSTM\",units = 256, enc_layers = 1, dec_layers = 1,embedding_dim = 32,dense_size=32,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim=len(english_tokens)+1,output_dim = embedding_dim,mask_zero=True)\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_tokens)+1,output_dim = embedding_dim,mask_zero=True)\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    attention = Attention(units)\n",
        "    tot_out = []\n",
        "    concat1 = Concatenate(axis=-1)\n",
        "    concat2 = Concatenate(axis=1)    \n",
        "    if cell == \"LSTM\":\n",
        "        encoder_prev = [LSTM(units,return_sequences=True) for i in range(enc_layers-1)]\n",
        "        encoder_fin = LSTM(units,return_sequences=True,return_state=True)\n",
        "        temp = encoder_context\n",
        "        for lay in encoder_prev:\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout)(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [LSTM(units,return_sequences=True,return_state=True) for i in range(dec_layers)]\n",
        "        temp_states = [dec_states]*dec_layers\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            context,att_wts = attention(temp_states[0][0],enc_out[0])\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "\n",
        "            for i in range(dec_layers):\n",
        "                temp,sh,sc = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = [sh,sc]\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "       \n",
        "    elif cell == \"GRU\":\n",
        "        encoder_prev = [GRU(units,return_sequences=True) for i in range(enc_layers-1)]\n",
        "        encoder_fin = GRU(units,return_sequences=True,return_state=True)\n",
        "        temp = encoder_context\n",
        "        for lay in encoder_prev:\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout)(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [GRU(units,return_sequences=True,return_state=True) for i in range(dec_layers)]\n",
        "        temp_states = []\n",
        "        for _ in range(dec_layers):\n",
        "            temp_states += dec_states\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            context,att_wts = attention(temp_states[0],enc_out[0])\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "\n",
        "            for i in range(dec_layers):\n",
        "                temp,st = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = st\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "            \n",
        "        \n",
        "    dense_lay1 = Dense(dense_size,activation='relu')\n",
        "    pre_out = dense_lay1(outt)\n",
        "    dense_lay2 = Dense(len(hindi_tokens)+1,activation = 'softmax')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    encoder_model = Model(encoder_inputs,enc_out)\n",
        "    \n",
        "    if cell == \"LSTM\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,units))\n",
        "        \n",
        "        temp = decoder_context\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            decoder_input_h = Input(shape=(units,))\n",
        "            decoder_input_c = Input(shape=(units,))\n",
        "            \n",
        "            if i==0:\n",
        "                context,att_wts_out = attention(decoder_input_h,encout_input)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,sh,sc = decoder[i](temp,initial_state = [decoder_input_h,decoder_input_c])\n",
        "            state_inputs += [decoder_input_h,decoder_input_c]\n",
        "            state_outputs += [sh,sc]\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "        \n",
        "    elif cell == \"GRU\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,units))\n",
        "        \n",
        "        temp = decoder_context\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            state_input = Input(shape=(units,))\n",
        "            \n",
        "            if i==0:\n",
        "                context,att_wts_out = attention(state_input,encout_input)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,s = decoder[i](temp,initial_state = state_input)\n",
        "            state_inputs.append(state_input)\n",
        "            state_outputs.append(s)\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "\n",
        "    pre_out = dense_lay1(temp)\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    decoder_model = Model(decoder_input_pass, [final_output,att_wts_out]+state_outputs)\n",
        "    \n",
        "    return train,encoder_model,decoder_model"
      ],
      "metadata": {
        "id": "pMjBcN1b1l1F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#configuring wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "%pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig6TYX0Sdooa",
        "outputId": "8e47670c-1e80-45a2-cb10-ac59cc2097a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdipnarayan501\u001b[0m (\u001b[33mfdl-moni_dip\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from wandb.keras import WandbCallback\n",
        "units = 256\n",
        "def train():\n",
        "    # Default values for hyper-parameters we're going to sweep over\n",
        "\n",
        "    config_defaults = {\n",
        "        'learning_rate': 1e-2,\n",
        "        'dense_size': 128,\n",
        "        'cell': 'LSTM',\n",
        "        'units' :256,\n",
        "        'embedding_dim': 64,\n",
        "        'enc_layers': 1,\n",
        "        'dec_layers': 1,\n",
        "        'dropout': 0.,\n",
        "        'batch_size': 64\n",
        "    }\n",
        "\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init(config=config_defaults)\n",
        "    \n",
        "    config = wandb.config\n",
        "\n",
        "    global units\n",
        "\n",
        "    enc_layers=config.enc_layers\n",
        "    dec_layers=config.dec_layers\n",
        "    embedding_dim = config.embedding_dim\n",
        "\n",
        "    dense_size=config.dense_size\n",
        "    cell = config.cell\n",
        "    dropout = config.dropout\n",
        "    learning_rate = config.learning_rate\n",
        "    batch_size = config.batch_size\n",
        "                                \n",
        "\n",
        "    # Displaying hyperparameters\n",
        "    run_name = \"{}_enc_lay_{}_dec_lay_{}_embd_{}_dp_{}_lr_{}_ds_{}_bs_{}\".format(cell, enc_layers, dec_layers,embedding_dim, dropout, learning_rate, dense_size,batch_size)\n",
        "    print(run_name)\n",
        " \n",
        "\n",
        "    \n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    #Build model\n",
        "    train,enc,dec = build_model(\n",
        "                                dense_size=dense_size,\n",
        "                                enc_layers=enc_layers,\n",
        "                                dec_layers=dec_layers,\n",
        "                                cell = cell,\n",
        "                                dropout = dropout,\n",
        "                                embedding_dim = embedding_dim)\n",
        "     \n",
        "     #Compliling model                           \n",
        "    train.compile(optimizer = Adam(learning_rate= learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    checkpoint = ModelCheckpoint('bestmodel.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
        "    #fitting model model\n",
        "    train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=batch_size,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=5,  #Change epoches here\n",
        "             callbacks = [WandbCallback(), checkpoint])\n",
        "\n",
        "\n",
        "\n",
        "    wandb.run.name = run_name\n",
        "    wandb.run.save()\n",
        "    return train"
      ],
      "metadata": {
        "id": "Dx_xaDYndorJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', #grid, random , bayes\n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'learning_rate': {\n",
        "            'values': [0.01, 0.001]\n",
        "        },\n",
        "        'dense_size': {\n",
        "            'values': [64,128,512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0.0,0.2,0.4]\n",
        "        },\n",
        "\n",
        "          'batch_size': {\n",
        "            'values': [64,128,256]\n",
        "        },\n",
        "        'cell': {\n",
        "            'values': [\"LSTM\",\"GRU\"]\n",
        "        },\n",
        "        'embedding_size': {\n",
        "            'values': [64,128,256]\n",
        "        },\n",
        "        'enc_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "                'units': {\n",
        "            'values': [256]\n",
        "        },\n",
        "        'dec_layers': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "#sweep_id = wandb.sweep(sweep_config, entity=\"fdl-moni_dip\", project=\"seq2seq_attention\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KXOWM--Idotm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCk52dDf_d7l",
        "outputId": "ff5bdb9e-a176-4974-f1ee-110e977584cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#id = 'q28nbq5b'\n",
        "wandb.agent(sweep_id, train,entity=\"fdl-moni_dip\", project=\"seq2seq_attention\", count=5)"
      ],
      "metadata": {
        "id": "HfAeNTHpdov7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a9mv2y1-doyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}