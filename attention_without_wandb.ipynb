{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipnarayan501/Assignment3/blob/main/attention_without_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6VgLpCUITeG0"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.layers import SimpleRNN,LSTM,GRU,Embedding,Dense,Dropout,Input,Concatenate\n",
        "from keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "daqcGfxprZ3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path to the dataset\n",
        "zip_path = \"drive/MyDrive/hi.zip\"\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q hi.zip"
      ],
      "metadata": {
        "id": "fuiaj-rwrfcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q42r5SX-anOe"
      },
      "outputs": [],
      "source": [
        "#Retrieving dataset\n",
        "def load_data(path):\n",
        "    with open(path) as fil:\n",
        "        data = pd.read_csv(fil,sep='\\t',header=None,names=[\"hi\",\"en\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[data['hi'].notna()]\n",
        "    data = data[data['en'].notna()]\n",
        "    data = data[['hi','en']]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OFs4DvTTtPB"
      },
      "outputs": [],
      "source": [
        "#Getteing data\n",
        "train = load_data(\"hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "dev = load_data(\"hi/lexicons/hi.translit.sampled.dev.tsv\")\n",
        "test = load_data(\"hi/lexicons/hi.translit.sampled.test.tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train['en'].values\n",
        "y = train['hi'].values\n",
        "y = '\\t'+y+'\\n'"
      ],
      "metadata": {
        "id": "njHmgQBG-pnJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bEnBqQAqT5h-"
      },
      "outputs": [],
      "source": [
        "#Getting unique tokens\n",
        "def unique_tokenize(train):\n",
        "    en = train['en'].values\n",
        "    hi = train['hi'].values\n",
        "    hi = '\\t'+hi+'\\n'\n",
        "    english_tokens = set()\n",
        "    hindi_tokens = set()\n",
        "    \n",
        "    for x,y in zip(en,hi):\n",
        "        for ch in x:\n",
        "            english_tokens.add(ch)\n",
        "        for ch in y:\n",
        "            hindi_tokens.add(ch)\n",
        "    english_tokens = sorted(list(english_tokens))\n",
        "    hindi_tokens = sorted(list(hindi_tokens))\n",
        "    return hindi_tokens , english_tokens\n",
        "hindi_tokens , english_tokens = unique_tokenize(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p1Bph1kNT6wA"
      },
      "outputs": [],
      "source": [
        "#Mapping tokens\n",
        "def tokenize_map(hindi_tokens , english_tokens):\n",
        "    eng_token_map = dict([(ch,i+1) for i,ch in enumerate(english_tokens)])\n",
        "    hin_token_map = dict([(ch,i+1) for i,ch in enumerate(hindi_tokens)])\n",
        "    hin_token_map[\" \"] = 0\n",
        "    eng_token_map[\" \"] = 0\n",
        "    return hin_token_map, eng_token_map\n",
        "\n",
        "hin_token_map, eng_token_map = tokenize_map(hindi_tokens , english_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting maximum tokens length\n",
        "max_eng_len = max([len(i) for i in x])\n",
        "max_hin_len = max([len(i) for i in y])"
      ],
      "metadata": {
        "id": "yR1xHVd59duE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PxgxWQwnT8D3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "reverse_eng_map = dict([(i,char) for char,i in eng_token_map.items()])\n",
        "reverse_hin_map = dict([(i,char) for char,i in hin_token_map.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "daeLD1QPanOj"
      },
      "outputs": [],
      "source": [
        "#Preprocessing data\n",
        "def process(data):\n",
        "    x,y = data['en'].values, data['hi'].values\n",
        "    y = \"\\t\" + y + \"\\n\"\n",
        "    \n",
        "    a = np.zeros((len(x),max_eng_len),dtype=\"float32\")\n",
        "    b = np.zeros((len(y),max_hin_len),dtype=\"float32\")\n",
        "    c = np.zeros((len(y),max_hin_len,len(hindi_tokens)+1),dtype=\"int\")\n",
        "    \n",
        "    \n",
        "    for i,(xx,yy) in enumerate(zip(x,y)):\n",
        "        for j,ch in enumerate(xx):\n",
        "            a[i,j] = eng_token_map[ch]\n",
        "\n",
        "        a[i,j+1:] = eng_token_map[\" \"]\n",
        "        for j,ch in enumerate(yy):\n",
        "            b[i,j] = hin_token_map[ch]\n",
        "\n",
        "            if j>0:\n",
        "                c[i,j-1,hin_token_map[ch]] = 1\n",
        "\n",
        "        b[i,j+1:] = hin_token_map[\" \"]\n",
        "        c[i,j:,hin_token_map[\" \"]] = 1\n",
        "        \n",
        "    return a,b,c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a8FMzKqvT9eA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Getting preprocess train val and test dataset\n",
        "trainx, trainxx, trainy = process(train)\n",
        "valx, valxx, valy = process(dev)\n",
        "testx,testxx,testy = process(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pZ7NBmt5anOk"
      },
      "outputs": [],
      "source": [
        "#reverse mapping\n",
        "np.random.seed(42)\n",
        "reverse_eng_map = dict([(i,char) for char,i in eng_token_map.items()])\n",
        "reverse_hin_map = dict([(i,char) for char,i in hin_token_map.items()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OBK-LOPcywvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zX5KFwbIanOm"
      },
      "outputs": [],
      "source": [
        "#Attention code here\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units,name=None, **kwargs):\n",
        "        super(Attention, self).__init__(name=name)\n",
        "        self.W1 = Dense(units,name='w1')\n",
        "        self.W2 = Dense(units,name='w2')\n",
        "        self.V = Dense(1,name='v')\n",
        "        self.units = units\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, tup):\n",
        "        # query hidden state \n",
        "        # query_with_time_axis \n",
        "        # values \n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query = tup[0]\n",
        "        values = tup[1]\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        #Getting scores\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector \n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super(Attention,self).get_config()\n",
        "        config.update({\n",
        "            'units': self.units\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cu5FewVsanOn"
      },
      "outputs": [],
      "source": [
        "#Generate encoder and decoders\n",
        "def build_model(cell = \"LSTM\",units = 32, enc_layers = 1, dec_layers = 1,embed_dim = 32,dense_size=256,dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = Input(shape=(None,),name='input1')\n",
        "    encoder_embedding = Embedding(input_dim=len(english_tokens)+1,output_dim = embed_dim,mask_zero=True,name=\"enc_embed\")\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = Input(shape=(None,),name='input2')\n",
        "    decoder_embedding = Embedding(input_dim = len(hindi_tokens)+1,output_dim = embed_dim,mask_zero=True,name=\"dec_embed\")\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    attention = Attention(units,name='attention')\n",
        "    tot_out = []\n",
        "    concat1 = Concatenate(axis=-1,name='concat1')\n",
        "    concat2 = Concatenate(axis=1,name='concat2')    \n",
        "    if cell == \"LSTM\":\n",
        "        encoder_prev = [LSTM(units,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = LSTM(units,return_sequences=True,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for x,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f'do_{x}')(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [LSTM(units,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        temp_states = [dec_states]*dec_layers\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            tup = (temp_states[0][0],enc_out[0])\n",
        "            context,att_wts = attention(tup)\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "\n",
        "            for i in range(dec_layers):\n",
        "                temp,sh,sc = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = [sh,sc]\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "       \n",
        "    elif cell == \"GRU\":\n",
        "        encoder_prev = [GRU(units,return_sequences=True,name=f\"enc_{i}\") for i in range(enc_layers-1)]\n",
        "        encoder_fin = GRU(units,return_sequences=True,return_state=True,name=f\"enc_{enc_layers-1}\")\n",
        "        temp = encoder_context\n",
        "        for x,lay in enumerate(encoder_prev):\n",
        "            temp = lay(temp)\n",
        "            if dropout is not None:\n",
        "                temp = Dropout(dropout,name=f'do_{x}')(temp)\n",
        "            \n",
        "        enc_out = encoder_fin(temp)\n",
        "        dec_states = enc_out[1:]\n",
        "        \n",
        "        decoder = [GRU(units,return_sequences=True,return_state=True,name=f\"dec_{i}\") for i in range(dec_layers)]\n",
        "        temp_states = []\n",
        "        for _ in range(dec_layers):\n",
        "            temp_states += dec_states\n",
        "        \n",
        "        for i in range(max_hin_len):\n",
        "            tup = (temp_states[0],enc_out[0])\n",
        "            context,att_wts = attention(tup)\n",
        "            temp = concat1([tf.expand_dims(context, 1), decoder_context[:,i:i+1,:]])\n",
        "\n",
        "            for i in range(dec_layers):\n",
        "                temp,st = decoder[i](temp,initial_state=temp_states[i])\n",
        "                temp_states[i] = st\n",
        "            tot_out.append(temp)\n",
        "            \n",
        "        outt = concat2(tot_out)\n",
        "            \n",
        "        \n",
        "    dense_lay1 = Dense(dense_size,activation='relu',name='dense1')\n",
        "    pre_out = dense_lay1(outt)\n",
        "    dense_lay2 = Dense(len(hindi_tokens)+1,activation = 'softmax',name='dense2')\n",
        "    final_output = dense_lay2(pre_out)\n",
        "    \n",
        "    train = Model([encoder_inputs,decoder_inputs],final_output)\n",
        "    \n",
        "    return train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGY_E1buanOo"
      },
      "outputs": [],
      "source": [
        "#Finding word level accuracy by substituting best parameters\n",
        "\n",
        "train = build_model(units=256,\n",
        "                    dense_size=128,\n",
        "                    enc_layers=1,\n",
        "                    dec_layers=1,\n",
        "                    cell = \"GRU\",\n",
        "                    dropout = 0.2,\n",
        "                    embed_dim = 256)\n",
        "train.compile(optimizer = 'adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "checkpoint= tf.keras.callbacks.ModelCheckpoint('best_model_attention.h5',monitor='val_accuracy',mode='max',save_best_only=True,verbose=1)\n",
        "train.fit([trainx,trainxx],trainy,\n",
        "             batch_size=64,\n",
        "             validation_data = ([valx,valxx],valy),\n",
        "             epochs=10,   #change epoches here\n",
        "             callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "W5Jqsw1nanOp"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "#Loading model\n",
        "model = keras.models.load_model('best_model_attention.h5',custom_objects={'accuracy':'accuracy','Attention':Attention})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UyfHfb8wanOp"
      },
      "outputs": [],
      "source": [
        "#Defining inference model\n",
        "def inference_models(model,nunits=32,enc_layers=1,dec_layers=1,cell='LSTM',dropout=None):\n",
        "    keras.backend.clear_session()\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_embedding = model.get_layer('enc_embed')\n",
        "    encoder_context = encoder_embedding(encoder_inputs)\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.get_layer('dec_embed')\n",
        "    decoder_context = decoder_embedding(decoder_inputs)\n",
        "    \n",
        "    \n",
        "    encoder_prev = [model.get_layer(f'enc_{i}') for i in range(enc_layers-1)]\n",
        "    encoder_fin = model.get_layer(f'enc_{enc_layers-1}')\n",
        "    temp = encoder_context\n",
        "    for i,lay in enumerate(encoder_prev):\n",
        "        temp = lay(temp)\n",
        "        if dropout is not None:\n",
        "            temp = model.get_layer(f'do_{i}')(temp)\n",
        "     \n",
        "    if cell == \"LSTM\":\n",
        "        enc_out, state_h,state_c = encoder_fin(temp)\n",
        "        enc_final = [enc_out,state_h,state_c]\n",
        "        \n",
        "    elif cell == \"GRU\":\n",
        "        enc_out, state = encoder_fin(temp)\n",
        "        enc_final = [enc_out,state]\n",
        "\n",
        "    encoder_model = keras.models.Model(encoder_inputs,enc_final)\n",
        "    \n",
        "    \n",
        "    decoder = [model.get_layer(f'dec_{i}') for i in range(dec_layers)]\n",
        "    \n",
        "    attention = model.get_layer('attention')\n",
        "    \n",
        "    concat1 = model.get_layer('concat1')\n",
        "\n",
        "    if cell == \"LSTM\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,nunits),name='inputenc')\n",
        "        \n",
        "        temp = decoder_context[:,-1:,:]\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            decoder_input_h = Input(shape=(nunits,),name=f\"inputh{i}\")\n",
        "            decoder_input_c = Input(shape=(nunits,),name=f\"inputc{i}\")\n",
        "            \n",
        "            if i==0:\n",
        "                tup = (decoder_input_h,encout_input)\n",
        "                context,att_wts_out = attention(tup)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,sh,sc = decoder[i](temp,initial_state = [decoder_input_h,decoder_input_c])\n",
        "            state_inputs += [decoder_input_h,decoder_input_c]\n",
        "            state_outputs += [sh,sc]\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "\n",
        "    elif cell == \"GRU\":\n",
        "        state_inputs = []\n",
        "        state_outputs = []\n",
        "        \n",
        "        encout_input = Input(shape=(None,nunits),name='inputenc')\n",
        "        \n",
        "        temp = decoder_context[:,:1,:]\n",
        "                                                                  \n",
        "        for i in range(dec_layers):\n",
        "            state_input = Input(shape=(nunits,),name=f\"inputs{i}\")\n",
        "            \n",
        "            if i==0:\n",
        "                tup = (state_input,encout_input)\n",
        "                context,att_wts_out = attention(tup)\n",
        "                temp = concat1([tf.expand_dims(context, 1), temp])\n",
        "                \n",
        "            temp,s = decoder[i](temp,initial_state = state_input)\n",
        "            state_inputs.append(state_input)\n",
        "            state_outputs.append(s)\n",
        "            \n",
        "        decoder_input_pass = [decoder_inputs,encout_input] + state_inputs\n",
        "\n",
        "    pre_out = model.get_layer('dense1')(temp)\n",
        "    final_output = model.get_layer('dense2')(pre_out)\n",
        "\n",
        "    decoder_model = keras.models.Model(decoder_input_pass, [final_output,att_wts_out]+state_outputs)\n",
        "    \n",
        "    return encoder_model,decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bFcjdhIzHzq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r0Txe9rvanOq"
      },
      "outputs": [],
      "source": [
        "#Initialising encoder and decoder model layer\n",
        "enc,dec = inference_models(train,nunits=256,enc_layers=1,dec_layers=1,cell=\"GRU\",dropout='yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHihui_XanOq"
      },
      "outputs": [],
      "source": [
        "#plot decode model\n",
        "keras.utils.plot_model(dec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Q9fFlIdsanOr"
      },
      "outputs": [],
      "source": [
        "#Beam search code here\n",
        "#It returns output word and attention weights\n",
        "def beam_search(inp,k,dec_layers,cell=\"LSTM\"):\n",
        "    enc_out = enc.predict(inp)\n",
        "    statess = enc_out[1:]\n",
        "    target_seq = np.zeros((inp.shape[0],1))\n",
        "    target_seq[:,0] = hin_token_map[\"\\t\"]\n",
        "    if cell == \"LSTM\":\n",
        "        states = []\n",
        "        for i in range(dec_layers):\n",
        "            states += [statess[0],statess[1]]\n",
        "    else:\n",
        "        states = []\n",
        "        for i in range(dec_layers):\n",
        "            states += [statess]\n",
        "            \n",
        "    output = dec.predict([target_seq,enc_out[0]]+states)\n",
        "    states = output[2:]\n",
        "    \n",
        "    stat1 = np.asarray(states).transpose([1,0,2])\n",
        "    \n",
        "    best_chars = np.argsort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "    scores = np.sort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "    sequences = [[([ch],-np.log(sc),stat1[i],0,output[1][i]) for ch,sc in zip(best_chars[i],scores[i])] for i in range(inp.shape[0])]\n",
        "    \n",
        "    for t1 in range(max_hin_len-1):\n",
        "        candidates = [[] for _ in range(inp.shape[0])]\n",
        "        for j in range(k):\n",
        "            target_seq[:,0] = [sequences[i][j][0][-1] for i in range(inp.shape[0])]\n",
        "            states = list(np.asarray([sequences[i][j][2] for i in range(inp.shape[0])]).transpose([1,0,2]))\n",
        "            output = dec.predict([target_seq,enc_out[0]]+states,batch_size=32)\n",
        "            best_chars = np.argsort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "            scores = np.sort(output[0][:,-1,:],axis=-1)[:,-k:]\n",
        "            \n",
        "            stat1 = np.asarray(output[2:]).transpose([1,0,2])\n",
        "            \n",
        "            for i in range(inp.shape[0]):\n",
        "                chk = 1 if (sequences[i][j][3]==1 or sequences[i][j][0][-1] == hin_token_map[\"\\n\"]) else 0\n",
        "                if chk == 0:\n",
        "                    candidates[i] += [(sequences[i][j][0]+[best_chars[i,rep]],\n",
        "                                       sequences[i][j][1]-np.log(scores[i,rep]),\n",
        "                                       stat1[i],\n",
        "                                       chk,\n",
        "                                       np.concatenate((sequences[i][j][4],output[1][i]),axis=1))\n",
        "                                      for rep in range(k)]\n",
        "                else:\n",
        "                    candidates[i] += [sequences[i][j]]\n",
        "                    \n",
        "        for i in range(inp.shape[0]):\n",
        "            candidates[i] = sorted(candidates[i],key = lambda tup:tup[1]/len(tup[0]))\n",
        "            sequences[i] = candidates[i][:k]\n",
        "            \n",
        "\n",
        "            \n",
        "        \n",
        "    res = [list() for i in range(inp.shape[0])]\n",
        "    att_wts = [list() for i in range(inp.shape[0])]\n",
        "    for i in range(inp.shape[0]):\n",
        "        for j in range(k):\n",
        "            res[i].append(sequences[i][j][0])\n",
        "            att_wts[i].append(sequences[i][j][4])\n",
        "        \n",
        "    return res,att_wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZvaveY1BanOr"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "#It might take a while\n",
        "#Predicting given test words using beam serach\n",
        "prediction,att_wts = beam_search(testx,5,1,cell=\"GRU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "tags": [],
        "id": "X7EbY86fanOr"
      },
      "outputs": [],
      "source": [
        "#Test accuracy with beam search\n",
        "def test_accuracy_beam(prediction):\n",
        "  acc = 0\n",
        "\n",
        "  for i,pre in enumerate(prediction):\n",
        "      chk = 0\n",
        "      \n",
        "      for pr in pre:\n",
        "          fl = 1\n",
        "          for j,ch in enumerate(pr):\n",
        "              if ch!=np.argmax(testy[i,j,:]):\n",
        "                  fl=0\n",
        "                  break\n",
        "              if ch==hin_token_map[\"\\n\"]:\n",
        "                  break\n",
        "          chk = chk or fl\n",
        "          \n",
        "      if chk==1:\n",
        "          acc+=1\n",
        "          \n",
        "          \n",
        "  return (acc/len(prediction))*100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7fVjyiAJanOs"
      },
      "outputs": [],
      "source": [
        "words_beam = []\n",
        "words = []\n",
        "#Getting test accuracy using first word of beam search\n",
        "def test_accuracy(prediction):\n",
        "  acc = 0\n",
        "\n",
        "  for i,pre in enumerate(prediction):\n",
        "      word = []\n",
        "      word1 = []\n",
        "      \n",
        "      orig = \"\"\n",
        "      for ch in testx[i]:\n",
        "          if reverse_eng_map[ch] == \" \":\n",
        "              break\n",
        "          orig += reverse_eng_map[ch]\n",
        "      word.append(orig)\n",
        "      word1.append(orig)\n",
        "      \n",
        "      hind = \"\"\n",
        "      for ch in testxx[i,1:]:\n",
        "          if reverse_hin_map[ch] == \"\\n\":\n",
        "              break\n",
        "          hind += reverse_hin_map[ch]\n",
        "      \n",
        "      word.append(hind)\n",
        "      word1.append(hind)\n",
        "      \n",
        "      fl=0\n",
        "      \n",
        "      for j,pr in enumerate(pre):\n",
        "          deco1 = \"\"\n",
        "          for ch in pr:\n",
        "              if reverse_hin_map[ch] == \"\\n\":\n",
        "                  break\n",
        "              deco1 += reverse_hin_map[ch]\n",
        "          word.append(deco1)\n",
        "          if j==0:\n",
        "              word1.append(deco1)\n",
        "              \n",
        "          \n",
        "              if hind==deco1:\n",
        "                  fl=1\n",
        "              \n",
        "      if fl==1:\n",
        "          acc += 1\n",
        "          \n",
        "      words.append(word1)\n",
        "      words_beam.append(word)\n",
        "      \n",
        "  return (acc/len(words))*100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test accuracy with beam search taking top 5 words is \", test_accuracy_beam(prediction))\n",
        "print(\"Test accuracy is taking first words\", test_accuracy(prediction))"
      ],
      "metadata": {
        "id": "Zgz_qD8KP5CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2nkaozC_anOs"
      },
      "outputs": [],
      "source": [
        "beam_df = pd.DataFrame(words_beam ,columns=['English','Hindi']+[f'Pred_Attention_{i}' for i in range(5)])\n",
        "test_df = pd.DataFrame(words,columns=['English','Hindi','pred_Attention'])\n",
        "test_df.to_csv('predictions_attention.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "NM5NsudASZEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScznJRRaanOs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3Lw-wnBanOs"
      },
      "source": [
        "## Question 5(d)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.font_manager import FontProperties\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "#Plotting attention heatmap code here\n",
        "def plot_attention_heatmap(attention, actual, predicted,orig,hind,deco):\n",
        "    \n",
        "    fig = plt.figure(figsize=(6,5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    plott = ax.matshow(attention, cmap='viridis')\n",
        "    h_font = FontProperties(fname = 'VesperLibre-Regular.ttf') #Upload VesperLibre-Regular.ttf   \n",
        "    ax.set_xticklabels([''] + actual, fontdict= {'fontsize': 16}, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted, fontproperties=h_font, fontdict={'fontsize': 16})\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))   \n",
        "    title = f\"Original word in english: {orig}\\nOriginal word in hindi: {hind}\\nDecoded word in hindi: {deco}\"   \n",
        "    fontdict = {'fontsize': 17}\n",
        "    plt.title(title,fontproperties=h_font, fontsize=14,y=-.3)   \n",
        "    divider = make_axes_locatable(ax)\n",
        "    c = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "    plt.colorbar(plott,cax=c)    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{orig}.png', bbox_inches = 'tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "m9Ef-VCFIYZH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = beam_df.values\n",
        "#Upload VesperLibre-Regular.ttf before  running\n",
        "#Printing heatmap for 10 words taking first predicted word\n",
        "for _ in range(9):\n",
        "    sp = np.random.choice(words.shape[0])\n",
        "    decoded_list = [x for x in words[sp][2].strip()] + ['<end>']\n",
        "    input_list = [x for x in words[sp][0].strip()] + ['<end>']\n",
        "    attention_plot = att_wts[sp][0][:len(input_list),:len(decoded_list)].T\n",
        "    print(\"Original word in english:\", words[sp][0])\n",
        "    print(\"Original word in hindi:\", words[sp][1])\n",
        "    print(\"Decoded word in hindi:\", words[sp][2])\n",
        "    plot_attention_heatmap(attention_plot, input_list, decoded_list,words[sp][0],words[sp][1],words[sp][2])"
      ],
      "metadata": {
        "id": "p-MgIXb_IbZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "\n",
        "# get html element\n",
        "def cstr(s, color='black'):\n",
        "    if s == ' ':\n",
        "        return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
        "    else:\n",
        "        return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n",
        "\n",
        "# print html\n",
        "def print_color(t,ch):\n",
        "    display(html_print(''.join([cstr(ch,color='#fffff')]+[cstr(' ',color='#fffff') for _ in range(3)]+[cstr(ti, color=ci) for ti,ci in t])))\n",
        "\n",
        "# get appropriate color for value\n",
        "def get_clr(value):\n",
        "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
        "     '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
        "        '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
        "       '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
        "    value = max(0,min(int((value * 100) / 5),19))\n",
        "    return colors[value]\n",
        "\n",
        "def sigmoid(x):\n",
        "    z = 1/(1 + np.exp(-x)) \n",
        "    return z\n",
        "\n",
        "def visualize(output, results,dec_char,dec):\n",
        "    text_colours = []\n",
        "    for i in range(len(output)):\n",
        "        text = (results[i], get_clr(output[i]))\n",
        "        text_colours.append(text)\n",
        "    print_color(text_colours,dec)"
      ],
      "metadata": {
        "id": "AJeOriWpIruH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualising word code here\n",
        "def visualise_connectivity(ex,sp):\n",
        "    word = words[ex][sp+2]\n",
        "    engword = words[ex][0]\n",
        "    attwts = att_wts[ex][sp][:len(engword),:]\n",
        "    print(\"Original word in english:\",engword)\n",
        "    print(\"Decoded word in hindi:\",word)\n",
        "    for i in range(len(word)):\n",
        "        visualize(attwts[:,i],engword,i,word[i])\n",
        "\n",
        "#visualise random word\n",
        "visualise_connectivity(np.random.choice(len(words)),0)    #Rerun "
      ],
      "metadata": {
        "id": "bB-1saCwIwC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "msOvevVhlpdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gif part"
      ],
      "metadata": {
        "id": "mm__SDKHmZM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download Lohit-devanagri .ttf \n",
        "!gdown --id 1g2bTbYAVLmc_IuKzTKOVqgLU7uvUE7dr "
      ],
      "metadata": {
        "id": "W8zt3qLwVPd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import cv2 as cv\n",
        "\n",
        "#Generating gif images for single word\n",
        "\n",
        "def visualise(word_index):\n",
        "\n",
        "  choosen_word = test_df.iloc[word_index,2]\n",
        "  print(choosen_word)\n",
        "  frames =[]\n",
        "\n",
        "  org = test_df.iloc[word_index,0]\n",
        "  warnings.filterwarnings('ignore')\n",
        "  font_c = FontProperties(fname = 'Lohit-Devanagari.ttf')\n",
        "  for i in range(len(choosen_word)):\n",
        "    plt.text(0,0.8,choosen_word,fontproperties=font_c,fontsize = 30)\n",
        "    plt.text(0.9,0.8,choosen_word[i],fontproperties=font_c,fontsize = 30)\n",
        "    plt.axis('off')\n",
        "\n",
        "    alphas = att_wts[word_index][i]\n",
        "    for j in range(len(org)):\n",
        "      t=plt.text(0.5*(j/6),0.5,org[j],fontsize=30)\n",
        "      t.set_bbox(dict(facecolor='blue', alpha=alphas[j][i], edgecolor='blue'))\n",
        "    plt.savefig('fig.png')\n",
        "    plt.show()\n",
        "    frames.append(cv.cvtColor(cv.imread('fig.png'), cv.COLOR_BGR2RGB ))  \n",
        "\n",
        "  clip = ImageSequenceClip(frames, fps=5)\n",
        "  clip.write_gif('word' + str(word_index)+'.gif', fps=5)\n",
        "  return clip"
      ],
      "metadata": {
        "id": "IWqJLp5dVPj8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LATYI_wkanOu"
      },
      "outputs": [],
      "source": [
        "#Generating gif image for random sample\n",
        "which_word = 20\n",
        "clip = visualise(which_word)  #Change the number here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BmL8nxVOmJS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "attention_without_wandb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}